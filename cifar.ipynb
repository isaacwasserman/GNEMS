{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/200], Step [200/391], Loss: 4.6028\n",
      "Epoch [2/200], Step [200/391], Loss: 4.5805\n",
      "Epoch [3/200], Step [200/391], Loss: 4.4705\n",
      "Epoch [4/200], Step [200/391], Loss: 4.1714\n",
      "Epoch [5/200], Step [200/391], Loss: 3.9977\n",
      "Epoch [6/200], Step [200/391], Loss: 3.8996\n",
      "Epoch [7/200], Step [200/391], Loss: 3.8208\n",
      "Epoch [8/200], Step [200/391], Loss: 3.7404\n",
      "Epoch [9/200], Step [200/391], Loss: 3.6654\n",
      "Epoch [10/200], Step [200/391], Loss: 3.6002\n",
      "Epoch [11/200], Step [200/391], Loss: 3.5355\n",
      "Epoch [12/200], Step [200/391], Loss: 3.4888\n",
      "Epoch [13/200], Step [200/391], Loss: 3.4349\n",
      "Epoch [14/200], Step [200/391], Loss: 3.4016\n",
      "Epoch [15/200], Step [200/391], Loss: 3.3482\n",
      "Epoch [16/200], Step [200/391], Loss: 3.2942\n",
      "Epoch [17/200], Step [200/391], Loss: 3.2501\n",
      "Epoch [18/200], Step [200/391], Loss: 3.2053\n",
      "Epoch [19/200], Step [200/391], Loss: 3.1667\n",
      "Epoch [20/200], Step [200/391], Loss: 3.1326\n",
      "Epoch [21/200], Step [200/391], Loss: 3.0971\n",
      "Epoch [22/200], Step [200/391], Loss: 3.0479\n",
      "Epoch [23/200], Step [200/391], Loss: 3.0092\n",
      "Epoch [24/200], Step [200/391], Loss: 2.9731\n",
      "Epoch [25/200], Step [200/391], Loss: 2.9445\n",
      "Epoch [26/200], Step [200/391], Loss: 2.9178\n",
      "Epoch [27/200], Step [200/391], Loss: 2.8776\n",
      "Epoch [28/200], Step [200/391], Loss: 2.8506\n",
      "Epoch [29/200], Step [200/391], Loss: 2.8324\n",
      "Epoch [30/200], Step [200/391], Loss: 2.7930\n",
      "Epoch [31/200], Step [200/391], Loss: 2.7625\n",
      "Epoch [32/200], Step [200/391], Loss: 2.7438\n",
      "Epoch [33/200], Step [200/391], Loss: 2.7328\n",
      "Epoch [34/200], Step [200/391], Loss: 2.7063\n",
      "Epoch [35/200], Step [200/391], Loss: 2.6751\n",
      "Epoch [36/200], Step [200/391], Loss: 2.6417\n",
      "Epoch [37/200], Step [200/391], Loss: 2.6293\n",
      "Epoch [38/200], Step [200/391], Loss: 2.6120\n",
      "Epoch [39/200], Step [200/391], Loss: 2.5913\n",
      "Epoch [40/200], Step [200/391], Loss: 2.5738\n",
      "Epoch [41/200], Step [200/391], Loss: 2.5605\n",
      "Epoch [42/200], Step [200/391], Loss: 2.5258\n",
      "Epoch [43/200], Step [200/391], Loss: 2.5254\n",
      "Epoch [44/200], Step [200/391], Loss: 2.5010\n",
      "Epoch [45/200], Step [200/391], Loss: 2.4905\n",
      "Epoch [46/200], Step [200/391], Loss: 2.4670\n",
      "Epoch [47/200], Step [200/391], Loss: 2.4508\n",
      "Epoch [48/200], Step [200/391], Loss: 2.4233\n",
      "Epoch [49/200], Step [200/391], Loss: 2.4274\n",
      "Epoch [50/200], Step [200/391], Loss: 2.3976\n",
      "Epoch [51/200], Step [200/391], Loss: 2.3776\n",
      "Epoch [52/200], Step [200/391], Loss: 2.3532\n",
      "Epoch [53/200], Step [200/391], Loss: 2.3470\n",
      "Epoch [54/200], Step [200/391], Loss: 2.3261\n",
      "Epoch [55/200], Step [200/391], Loss: 2.3098\n",
      "Epoch [56/200], Step [200/391], Loss: 2.3007\n",
      "Epoch [57/200], Step [200/391], Loss: 2.2862\n",
      "Epoch [58/200], Step [200/391], Loss: 2.2613\n",
      "Epoch [59/200], Step [200/391], Loss: 2.2494\n",
      "Epoch [60/200], Step [200/391], Loss: 2.2454\n",
      "Epoch [61/200], Step [200/391], Loss: 2.2065\n",
      "Epoch [62/200], Step [200/391], Loss: 2.2005\n",
      "Epoch [63/200], Step [200/391], Loss: 2.1852\n",
      "Epoch [64/200], Step [200/391], Loss: 2.1490\n",
      "Epoch [65/200], Step [200/391], Loss: 2.1558\n",
      "Epoch [66/200], Step [200/391], Loss: 2.1317\n",
      "Epoch [67/200], Step [200/391], Loss: 2.1349\n",
      "Epoch [68/200], Step [200/391], Loss: 2.1033\n",
      "Epoch [69/200], Step [200/391], Loss: 2.0958\n",
      "Epoch [70/200], Step [200/391], Loss: 2.0903\n",
      "Epoch [71/200], Step [200/391], Loss: 2.0770\n",
      "Epoch [72/200], Step [200/391], Loss: 2.0737\n",
      "Epoch [73/200], Step [200/391], Loss: 2.0476\n",
      "Epoch [74/200], Step [200/391], Loss: 2.0458\n",
      "Epoch [75/200], Step [200/391], Loss: 2.0354\n",
      "Epoch [76/200], Step [200/391], Loss: 2.0031\n",
      "Epoch [77/200], Step [200/391], Loss: 2.0015\n",
      "Epoch [78/200], Step [200/391], Loss: 2.0013\n",
      "Epoch [79/200], Step [200/391], Loss: 1.9808\n",
      "Epoch [80/200], Step [200/391], Loss: 1.9780\n",
      "Epoch [81/200], Step [200/391], Loss: 1.9599\n",
      "Epoch [82/200], Step [200/391], Loss: 1.9526\n",
      "Epoch [83/200], Step [200/391], Loss: 1.9345\n",
      "Epoch [84/200], Step [200/391], Loss: 1.9149\n",
      "Epoch [85/200], Step [200/391], Loss: 1.9177\n",
      "Epoch [86/200], Step [200/391], Loss: 1.8966\n",
      "Epoch [87/200], Step [200/391], Loss: 1.8893\n",
      "Epoch [88/200], Step [200/391], Loss: 1.8875\n",
      "Epoch [89/200], Step [200/391], Loss: 1.8639\n",
      "Epoch [90/200], Step [200/391], Loss: 1.8566\n",
      "Epoch [91/200], Step [200/391], Loss: 1.8514\n",
      "Epoch [92/200], Step [200/391], Loss: 1.8153\n",
      "Epoch [93/200], Step [200/391], Loss: 1.8246\n",
      "Epoch [94/200], Step [200/391], Loss: 1.8246\n",
      "Epoch [95/200], Step [200/391], Loss: 1.8104\n",
      "Epoch [96/200], Step [200/391], Loss: 1.7935\n",
      "Epoch [97/200], Step [200/391], Loss: 1.8143\n",
      "Epoch [98/200], Step [200/391], Loss: 1.7861\n",
      "Epoch [99/200], Step [200/391], Loss: 1.7648\n",
      "Epoch [100/200], Step [200/391], Loss: 1.7613\n",
      "Epoch [101/200], Step [200/391], Loss: 1.7482\n",
      "Epoch [102/200], Step [200/391], Loss: 1.7458\n",
      "Epoch [103/200], Step [200/391], Loss: 1.7363\n",
      "Epoch [104/200], Step [200/391], Loss: 1.7298\n",
      "Epoch [105/200], Step [200/391], Loss: 1.7172\n",
      "Epoch [106/200], Step [200/391], Loss: 1.7190\n",
      "Epoch [107/200], Step [200/391], Loss: 1.7177\n",
      "Epoch [108/200], Step [200/391], Loss: 1.6882\n",
      "Epoch [109/200], Step [200/391], Loss: 1.7099\n",
      "Epoch [110/200], Step [200/391], Loss: 1.6928\n",
      "Epoch [111/200], Step [200/391], Loss: 1.6772\n",
      "Epoch [112/200], Step [200/391], Loss: 1.6621\n",
      "Epoch [113/200], Step [200/391], Loss: 1.6526\n",
      "Epoch [114/200], Step [200/391], Loss: 1.6487\n",
      "Epoch [115/200], Step [200/391], Loss: 1.6443\n",
      "Epoch [116/200], Step [200/391], Loss: 1.6260\n",
      "Epoch [117/200], Step [200/391], Loss: 1.6193\n",
      "Epoch [118/200], Step [200/391], Loss: 1.6087\n",
      "Epoch [119/200], Step [200/391], Loss: 1.6077\n",
      "Epoch [120/200], Step [200/391], Loss: 1.5915\n",
      "Epoch [121/200], Step [200/391], Loss: 1.5860\n",
      "Epoch [122/200], Step [200/391], Loss: 1.5899\n",
      "Epoch [123/200], Step [200/391], Loss: 1.5763\n",
      "Epoch [124/200], Step [200/391], Loss: 1.5697\n",
      "Epoch [125/200], Step [200/391], Loss: 1.5596\n",
      "Epoch [126/200], Step [200/391], Loss: 1.5550\n",
      "Epoch [127/200], Step [200/391], Loss: 1.5444\n",
      "Epoch [128/200], Step [200/391], Loss: 1.5438\n",
      "Epoch [129/200], Step [200/391], Loss: 1.5232\n",
      "Epoch [130/200], Step [200/391], Loss: 1.5243\n",
      "Epoch [131/200], Step [200/391], Loss: 1.5112\n",
      "Epoch [132/200], Step [200/391], Loss: 1.5233\n",
      "Epoch [133/200], Step [200/391], Loss: 1.5072\n",
      "Epoch [134/200], Step [200/391], Loss: 1.4993\n",
      "Epoch [135/200], Step [200/391], Loss: 1.4958\n",
      "Epoch [136/200], Step [200/391], Loss: 1.4752\n",
      "Epoch [137/200], Step [200/391], Loss: 1.4725\n",
      "Epoch [138/200], Step [200/391], Loss: 1.4719\n",
      "Epoch [139/200], Step [200/391], Loss: 1.4747\n",
      "Epoch [140/200], Step [200/391], Loss: 1.4746\n",
      "Epoch [141/200], Step [200/391], Loss: 1.4539\n",
      "Epoch [142/200], Step [200/391], Loss: 1.4578\n",
      "Epoch [143/200], Step [200/391], Loss: 1.4371\n",
      "Epoch [144/200], Step [200/391], Loss: 1.4294\n",
      "Epoch [145/200], Step [200/391], Loss: 1.4253\n",
      "Epoch [146/200], Step [200/391], Loss: 1.4033\n",
      "Epoch [147/200], Step [200/391], Loss: 1.4153\n",
      "Epoch [148/200], Step [200/391], Loss: 1.4157\n",
      "Epoch [149/200], Step [200/391], Loss: 1.3891\n",
      "Epoch [150/200], Step [200/391], Loss: 1.3906\n",
      "Epoch [151/200], Step [200/391], Loss: 1.3819\n",
      "Epoch [152/200], Step [200/391], Loss: 1.3824\n",
      "Epoch [153/200], Step [200/391], Loss: 1.3689\n",
      "Epoch [154/200], Step [200/391], Loss: 1.3751\n",
      "Epoch [155/200], Step [200/391], Loss: 1.3659\n",
      "Epoch [156/200], Step [200/391], Loss: 1.3694\n",
      "Epoch [157/200], Step [200/391], Loss: 1.3597\n",
      "Epoch [158/200], Step [200/391], Loss: 1.3546\n",
      "Epoch [159/200], Step [200/391], Loss: 1.3535\n",
      "Epoch [160/200], Step [200/391], Loss: 1.3315\n",
      "Epoch [161/200], Step [200/391], Loss: 1.3300\n",
      "Epoch [162/200], Step [200/391], Loss: 1.3072\n",
      "Epoch [163/200], Step [200/391], Loss: 1.3172\n",
      "Epoch [164/200], Step [200/391], Loss: 1.3085\n",
      "Epoch [165/200], Step [200/391], Loss: 1.2978\n",
      "Epoch [166/200], Step [200/391], Loss: 1.2889\n",
      "Epoch [167/200], Step [200/391], Loss: 1.2904\n",
      "Epoch [168/200], Step [200/391], Loss: 1.2880\n",
      "Epoch [169/200], Step [200/391], Loss: 1.2802\n",
      "Epoch [170/200], Step [200/391], Loss: 1.2683\n",
      "Epoch [171/200], Step [200/391], Loss: 1.2626\n",
      "Epoch [172/200], Step [200/391], Loss: 1.2573\n",
      "Epoch [173/200], Step [200/391], Loss: 1.2661\n",
      "Epoch [174/200], Step [200/391], Loss: 1.2396\n",
      "Epoch [175/200], Step [200/391], Loss: 1.2421\n",
      "Epoch [176/200], Step [200/391], Loss: 1.2351\n",
      "Epoch [177/200], Step [200/391], Loss: 1.2329\n",
      "Epoch [178/200], Step [200/391], Loss: 1.2348\n",
      "Epoch [179/200], Step [200/391], Loss: 1.2231\n",
      "Epoch [180/200], Step [200/391], Loss: 1.2124\n",
      "Epoch [181/200], Step [200/391], Loss: 1.2137\n",
      "Epoch [182/200], Step [200/391], Loss: 1.1937\n",
      "Epoch [183/200], Step [200/391], Loss: 1.2027\n",
      "Epoch [184/200], Step [200/391], Loss: 1.2062\n",
      "Epoch [185/200], Step [200/391], Loss: 1.1866\n",
      "Epoch [186/200], Step [200/391], Loss: 1.2023\n",
      "Epoch [187/200], Step [200/391], Loss: 1.2007\n",
      "Epoch [188/200], Step [200/391], Loss: 1.1842\n",
      "Epoch [189/200], Step [200/391], Loss: 1.1788\n",
      "Epoch [190/200], Step [200/391], Loss: 1.1654\n",
      "Epoch [191/200], Step [200/391], Loss: 1.1576\n",
      "Epoch [192/200], Step [200/391], Loss: 1.1637\n",
      "Epoch [193/200], Step [200/391], Loss: 1.1490\n",
      "Epoch [194/200], Step [200/391], Loss: 1.1414\n",
      "Epoch [195/200], Step [200/391], Loss: 1.1343\n",
      "Epoch [196/200], Step [200/391], Loss: 1.1411\n",
      "Epoch [197/200], Step [200/391], Loss: 1.1188\n",
      "Epoch [198/200], Step [200/391], Loss: 1.1280\n",
      "Epoch [199/200], Step [200/391], Loss: 1.1299\n",
      "Epoch [200/200], Step [200/391], Loss: 1.1131\n",
      "Training finished!\n",
      "Accuracy of the model on the 10000 test images: 47.49%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the small CNN model\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"mps\"\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the model\n",
    "model = SmallCNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Step [{i+1}/{len(trainloader)}], '\n",
    "                  f'Loss: {running_loss / 200:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the 10000 test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "# save full model with weights\n",
    "torch.save(model, \"cifar100_full.pt\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
